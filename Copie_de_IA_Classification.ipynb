{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Connexion au Drive où se trouvent les données**"
      ],
      "metadata": {
        "id": "wC6K9qPDP79c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "KynwAXG_sLvc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Chemin d'accès au dossier 'Data' contenant les genres**"
      ],
      "metadata": {
        "id": "6Q1MYC_lQooC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = 'your_path_to_drive/Data/genres_original'\n",
        "genres = os.listdir(data_path)"
      ],
      "metadata": {
        "id": "78chDGaLQvyr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Installation des librairies**"
      ],
      "metadata": {
        "id": "YRiEwehXQGo5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import librosa\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Dense, Dropout, LSTM\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "\n",
        "!pip install resampy\n",
        "!pip install pydub\n",
        "\n",
        "import soundfile as sf\n",
        "from pydub import AudioSegment"
      ],
      "metadata": {
        "id": "Kns3WXS0JxMp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Extraction des caractéristiques MFCC et préparation des données**"
      ],
      "metadata": {
        "id": "To4gRfSEQfqn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fonction pour extraire les caractéristiques d'un fichier audio\n",
        "def extract_features(file_path):\n",
        "    try:\n",
        "        # Charger le fichier audio\n",
        "        audio, sample_rate = librosa.load(file_path, res_type='kaiser_fast')\n",
        "        # Extraire les MFCC\n",
        "        mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)\n",
        "        # Moyenne des coefficients MFCC\n",
        "        mfccs_processed = np.mean(mfccs.T,axis=0)\n",
        "    except Exception as e:\n",
        "        print(\"Erreur rencontrée lors de l'extraction des caractéristiques: \", e)\n",
        "        return None\n",
        "    return mfccs_processed"
      ],
      "metadata": {
        "id": "pQnZ01JmQzip"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Préparer les données et les étiquettes**"
      ],
      "metadata": {
        "id": "9YYDI4qyRQba"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "features = []\n",
        "labels = []"
      ],
      "metadata": {
        "id": "l5PrBYDNRVKh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Extraction des caractéristiques pour chaque fichier audio de chaque catégorie musicale**"
      ],
      "metadata": {
        "id": "MAbadjUrTkhL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for genre in genres:\n",
        "    # Chemin d'accès aux fichiers pour ce genre\n",
        "    file_paths = os.listdir(os.path.join(data_path, genre))\n",
        "    for file in file_paths:\n",
        "        file_path = os.path.join(data_path, genre, file)\n",
        "        # Extraire les caractéristiques pour chaque fichier audio\n",
        "        data = extract_features(file_path)\n",
        "        if data is not None:\n",
        "            features.append(data)\n",
        "            labels.append(genre)\n",
        "        else:\n",
        "            print(f\"Le fichier {file} a été ignoré en raison d'une erreur de lecture.\")"
      ],
      "metadata": {
        "id": "ydzmg1NvUmJZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conversion de la liste des caractéristiques en un tableau numpy**"
      ],
      "metadata": {
        "id": "TYZBCa5fUsfs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "features_array = np.array(features)"
      ],
      "metadata": {
        "id": "W4eLQZfRU2Nc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Encodage des étiquettes**"
      ],
      "metadata": {
        "id": "afFoykpeU4wJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "le = LabelEncoder()\n",
        "labels_encoded = le.fit_transform(labels)\n",
        "labels_categorical = to_categorical(labels_encoded)\n",
        "num_classes = labels_categorical.shape[1]"
      ],
      "metadata": {
        "id": "xSVxkPLqVAlp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Division des données en ensembles d'entraînement et de test**"
      ],
      "metadata": {
        "id": "uICul6TIVNrD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(features_array, labels_categorical, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "5a40CRcZVYsv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Redimensionnement des données pour le CNN**"
      ],
      "metadata": {
        "id": "xdKj0vYgVhf5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_reshaped = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
        "X_test_reshaped = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)"
      ],
      "metadata": {
        "id": "OBsMjfRt5pJD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Construction du Modèle CNN-LSTM pour la Classification des Genres Musicaux**"
      ],
      "metadata": {
        "id": "vD7p2Ue6WdPO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialisation d'un modèle séquentiel dans Keras\n",
        "# Modèle de réseau de neurone\n",
        "model = Sequential()\n",
        "\n",
        "# Couches Conv1D\n",
        "model.add(Conv1D(filters=16, kernel_size=2, activation='relu', input_shape=(X_train_reshaped.shape[1], 1)))\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "# La transition de Conv1D à LSTM nécessite un \"aplatissage\" temporel, pas un Flatten() qui aplatirait tout\n",
        "# Le modèle Conv1D retourne déjà des données dans le format attendu par LSTM, donc aucune opération supplémentaire n'est nécessaire\n",
        "\n",
        "# Couche LSTM\n",
        "model.add(LSTM(64, return_sequences=False))  # return_sequences=False car la sortie suivante est une couche Dense\n",
        "\n",
        "# Couche Dense pour la classification\n",
        "model.add(Dense(num_classes, activation='softmax'))  # num_classes doit être défini en fonction du nombre de genres\n",
        "\n",
        "# Compiler le modèle\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Afficher le résumé du modèle\n",
        "model.summary()\n"
      ],
      "metadata": {
        "id": "nRpXkxG6_23Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Entraînement du modèle**"
      ],
      "metadata": {
        "id": "8ukT0s6JCN7q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Callback pour sauvegarder le meilleur modèle basé sur la précision de validation\n",
        "checkpoint = ModelCheckpoint(\"best_model.hdf5\", monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
        "\n",
        "# Callback pour arrêter l'entraînement si aucune amélioration en précision de validation n'est observée après 'patience' époques\n",
        "early_stopping = EarlyStopping(monitor='val_accuracy', patience=5, verbose=1, mode='max')\n",
        "\n",
        "# Entraîner le modèle\n",
        "history = model.fit(\n",
        "    X_train_reshaped,\n",
        "    y_train,\n",
        "    epochs=50,  # ajustable pour plus de précsion\n",
        "    batch_size=32,  # ajustable pour plus de précsion\n",
        "    validation_split=0.2,  # fraction des données pour la validation\n",
        "    callbacks=[checkpoint, early_stopping]\n",
        ")"
      ],
      "metadata": {
        "id": "bfAzKb6ZAWeQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Évaluation du modèle**"
      ],
      "metadata": {
        "id": "Yb6JuDPaCXk5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Charger le meilleur modèle sauvegardé\n",
        "model.load_weights(\"best_model.hdf5\")\n",
        "\n",
        "# Évaluer le modèle sur l'ensemble de test\n",
        "test_loss, test_accuracy = model.evaluate(X_test_reshaped, y_test, verbose=1)\n",
        "print(f\"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}\")\n"
      ],
      "metadata": {
        "id": "krS3n72mCLrb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Test existence fichier test**"
      ],
      "metadata": {
        "id": "20IVrWZCY_H-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = 'your_path_to_an_audio_file.mp3'\n",
        "print(os.path.exists(file_path))"
      ],
      "metadata": {
        "id": "gwq9YY3wGjQA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Transformatioh fichier mp3 en wav**"
      ],
      "metadata": {
        "id": "7Wx8TvCEZTUd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_path_mp3 = 'your_path_to_an_audio_file.mp3'\n",
        "file_path_wav = 'your_path_to_an_audio_file.wav'\n",
        "\n",
        "# Convertir mp3 en wav\n",
        "audio = AudioSegment.from_mp3(file_path_mp3)\n",
        "audio.export(file_path_wav, format=\"wav\")\n",
        "\n",
        "# Vérifiez que le fichier wav est bien créé\n",
        "print(os.path.exists(file_path_wav))\n"
      ],
      "metadata": {
        "id": "vfIWJysKI4nG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Prédictions avec le modèle**"
      ],
      "metadata": {
        "id": "mky98jQeDEgL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_genre(file_path, model, le):\n",
        "    # Extraire les caractéristiques du fichier audio\n",
        "    features = extract_features(file_path)\n",
        "    if features is not None:\n",
        "        features_reshaped = features.reshape(1, features.shape[0], 1)\n",
        "\n",
        "        # Faire la prédiction en utilisant le modèle\n",
        "        prediction = model.predict(features_reshaped)\n",
        "\n",
        "        # Convertir la prédiction en nom de genre\n",
        "        predicted_genre_index = np.argmax(prediction)\n",
        "        predicted_genre = le.inverse_transform([predicted_genre_index])\n",
        "\n",
        "        return predicted_genre[0]\n",
        "    else:\n",
        "        return \"Erreur lors de l'extraction des caractéristiques du fichier audio.\"\n",
        "\n",
        "# Utiliser la fonction pour prédire le genre d'un fichier audio\n",
        "file_path = 'your_path_to_an_audio_file.wav'\n",
        "genre_pred = predict_genre(file_path, model, le)\n",
        "print(f\"Le genre prédit pour l'extrait musical est : {genre_pred}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZNmaPJ_3DHCj",
        "outputId": "ab0191e0-a554-425e-87c6-e88afb0d6f68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 473ms/step\n",
            "Le genre prédit pour l'extrait musical est : metal\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Données utilisées](https://www.kaggle.com/datasets/andradaolteanu/gtzan-dataset-music-genre-classification)"
      ],
      "metadata": {
        "id": "cDR74FTCRgrW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Rapport**\n",
        "\n",
        "Au cours de ce projet, nous avons développé un modèle de machine learning pour la classification des genres musicaux. Notre approche s'est basée sur une architecture hybride combinant CNN et LSTM pour traiter efficacement les caractéristiques temporelles et spatiales des signaux audio. Bien que le modèle ait montré une certaine capacité à discriminer entre différents genres, l'accuracy obtenue de 51% indique une marge significative d'amélioration.\n",
        "\n",
        "Pour améliorer les performances du modèle, nous avons envisagé plusieurs stratégies :\n",
        "\n",
        "1. Augmentation et Diversification des Données : L'intégration de davantage\n",
        "d'échantillons musicaux, ainsi que l'application de techniques d'augmentation des données audio, telles que le changement de vitesse et l'ajout de bruit, pour enrichir notre ensemble d'apprentissage et augmenter la robustesse du modèle.\n",
        "\n",
        "2. Optimisation de l'Architecture du Modèle : Ajustement des couches et des neurones dans les couches Conv1D et LSTM, et expérimentation avec l'ajout de couches de normalisation par lots et de couches de dropout pour une meilleure généralisation.\n",
        "\n",
        "3. Ajustement Fin des Hyperparamètres : Modification du taux d'apprentissage, de la taille des lots, et du nombre d'époques, en utilisant des méthodes telles que la recherche par grille ou la recherche aléatoire pour trouver la combinaison optimale.\n",
        "\n",
        "4. Application de Techniques de Régularisation : Mise en œuvre de la régularisation L1/L2 et de l'early stopping pour contrer le surajustement.\n",
        "\n",
        "5. Évaluation Complète : Utilisation de métriques supplémentaires comme le F1-score et la matrice de confusion pour une évaluation plus nuancée, et application de la validation croisée pour assurer la stabilité et la fiabilité du modèle.\n",
        "\n",
        "Tout au long du projet, nous avons rencontré des défis tels que l'adaptation des données aux exigences structurelles du modèle et la gestion des fichiers audio mal lus ou corrompus. Ces défis ont été surmontés grâce à un prétraitement méticuleux des données et à une gestion robuste des exceptions lors de l'extraction des caractéristiques.\n",
        "\n",
        "En conclusion, bien que des améliorations soient nécessaires pour atteindre une précision plus élevée, les stratégies proposées et les leçons tirées de ce projet posent les bases pour de futures itérations et optimisations du modèle.\n",
        "\n",
        "**Arthur Liot & Paolig Blan**"
      ],
      "metadata": {
        "id": "wyrEHeebSPbW"
      }
    }
  ]
}